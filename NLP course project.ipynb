{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USA Presidents speeches classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project is made for purpose of Natural language processing course.<br>\n",
    "The goal of this project is to implement basic concepts of NLP and applied machine learning techniques.<br>\n",
    "## Project idea\n",
    "Datasets: speeches by 3 American presidents Bush , Obama and Trump.<br>\n",
    "Goal: train the system to recognize who is the author of the speech for a  given text.<br>\n",
    "How do we do that?<br>\n",
    "Supervised machine learning<br>\n",
    "**Multinominal logistic regression using stohastic gradient descent for optimizing loss function**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression is probabilistic linear classifier and it belongs to the class of discriminative classifiers. It means the system is trained to find features that make good discrimination between classes although it is not able to construct samples of each class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Components**:<br>\n",
    "1. feature representation - for each input speech , find next features:<br>\n",
    "-log(length(speech))<br>\n",
    "-number of positive words<br>\n",
    "-number of negative words<br>\n",
    "-number of word \"American(s)\" <br>\n",
    "-number of words from \"special words\" list<br>\n",
    "-number of the word \"Mexicans\" show up in the speech<br>\n",
    "2. softmax function - calculating probabilities for each class\n",
    "3. cost function - cross-entropy cost function for minimazing model error\n",
    "4. stohastic gradient descent - algorithm for minimazing loss function , finding the best weights for each class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from random import shuffle,uniform\n",
    "from math import e , pow , log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using NLTK library for tokenizing all words from english text\n",
    "def tokenizeWords(text):\n",
    "    pattern = r'(?:[A-Za-z]{1,3}\\.)+|\\w+(?:[-\\'_]\\w+)*|\\$?\\d+(?:\\.\\d+)*%?'\n",
    "    return regexp_tokenize(text,pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classic list of English stopwords\n",
    "def stop_words():\n",
    "    text = open(\"english.stop\",\"r\",encoding = \"utf8\").read()\n",
    "    return text.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(stop_words()) #English stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filename):\n",
    "    return open(filename,\"r\").read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import positive and negative sentiment words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_words = read_file(\"English_sentiment/positive.txt\").split(\"\\n\")\n",
    "negative_words = read_file(\"English_sentiment/negative.txt\").split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\"Trump\",\"Obama\",\"Bush\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Special words for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_words = [\"war\",\"terrorism\",\"money\",\"politics\",\"politician\",\"president\",\"democrat\",\"republican\",\"present\",\"healthcare\",\n",
    "                 \"Iraq\",\"strong\",\"stronger\",\"stupid\",\"tremendous\",\"winning\",\"amazing\",\"great\",\"military\",\"classy\",\"security\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scalar product used for softmax function for vectors\n",
    "$ x=(x_1,...,x_n) , y=(y_1,...,y_n) $ the result is $\\sum_{k=1}^n x_k y_k$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scalar(x,y):\n",
    "    return sum(a*b for a,b in zip(x,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax function\n",
    "Softmax function is used for calculating probabilities that input x belongs to each class.\n",
    "For input vector  $z=(z_1,...,z_n)$  softmax function is defined with $softmax(z) = ( \\frac{e^{z_1}}{\\sum_{k=1}^n e^{z_k}},...,\\frac{e^{z_n}}{\\sum_{k=1}^n e^{z_k}} )$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for vector z=(z_1,...,z_n)\n",
    "def softmax(z):\n",
    "    l = len(z)\n",
    "    a = sum(pow(e,x) for x in z)\n",
    "    probs = [0]*l #vector of probabilities for each class\n",
    "    for i in range(0,l):\n",
    "        probs[i] = pow(e,z[i]) / a\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import datasets\n",
    "Trump dataset contains 82 speeches , Obama dataset contains 48 speeches and Bush dataset contains 39 speeches.<br>In the first case 80% of all speeches will be used for training and the rest for testing. It means 31 for Bush , 39 for Obama and 66 for Trump are used as training examples.<br> In the second case equal number of data samples are import in the program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#datasets , each list is list of words for one speech\n",
    "trump = []\n",
    "obama = []\n",
    "bush = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if equal = True , we import equal number of samples of each class.Otherwise import all samples , stopwords cut\n",
    "def import_data(equal = False):\n",
    "    trump = []\n",
    "    obama = []\n",
    "    bush = []\n",
    "    #for trump\n",
    "    for i in range(1,83):\n",
    "        trump.append(list(set(tokenizeWords(read_file(\"Trump\\Trump (\"+str(i)+\").txt\")))-stopwords)) #append list of words for one speech\n",
    "    #for obama\n",
    "    for i in range(1,49):\n",
    "        obama.append(list(set(tokenizeWords(read_file(\"Obama\\Obama (\"+str(i)+\").txt\")))-stopwords))\n",
    "    #for bush\n",
    "    for i in range(1,40):\n",
    "        bush.append(list(set(tokenizeWords(read_file(\"Bush\\Bush (\"+str(i)+\").txt\")))-stopwords))\n",
    "    shuffle(trump)\n",
    "    shuffle(obama)\n",
    "    shuffle(bush)\n",
    "    if equal:\n",
    "        return trump[:40],obama[:40],bush\n",
    "    return trump,obama,bush"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump,obama,bush = import_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features\n",
    "For each dataset one of first steps to do is normalization to cut out stopwords from the speech for easier computation. After that it is easy to get count of positive/negative sentiment words and count of words from special list.<br>For this problem features are linguistic characteristics of text.\n",
    "Feature is a numerical vector of the following:<br>\n",
    "**feature = ( log(len(speech)),number_positive_words,number_negative_words,number_special_words,number_usa_words,number_mexico_words )**.<br>\n",
    "Below is the function for getting features of any single speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to get features from  one speech\n",
    "def features(text):# text is the list of words from 1 speech\n",
    "    f = [0]*6 #vector of features\n",
    "    f[0] = log(len(text),10) #first feature is log(length(speech)) , base = 10\n",
    "    pos,neg,special,america,mexico = 0,0,0,0,0\n",
    "    for word in text:\n",
    "        if word in positive_words:\n",
    "            pos += 1\n",
    "        elif word in negative_words:\n",
    "            neg += 1\n",
    "        if word in special_words:\n",
    "            special += 1\n",
    "        w = word.lower()\n",
    "        if w in [\"american\",\"americans\",\"america\",\"usa\",\"u.s.a\"]:\n",
    "            america += 1\n",
    "        elif w in [\"mexico\",\"mexicans\"]:\n",
    "            mexico += 1\n",
    "    f[1] = pos\n",
    "    f[2] = neg\n",
    "    f[3] = special\n",
    "    f[4] = america\n",
    "    f[5] = mexico\n",
    "    return f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before trainig , data must be split into training examples and test examples and test examples. 80% of all data is used for training. Random selection is used to pick which speeches belong to training set. After that we need to mark training data with particular class it belongs to and turn text into features.<br>\n",
    "Training samples are pairs of vector of features form particular sample and true class it belongs to.<br>\n",
    "Test samples are just unmarked vectors of features from dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(equal = False):\n",
    "    train = [] #train-set\n",
    "    test = [] #test-set\n",
    "    a = 65\n",
    "    b = 38\n",
    "    t = len(trump)\n",
    "    o = len(obama)\n",
    "    if equal:\n",
    "        a,b = 29,29\n",
    "        t,o = 38,38\n",
    "    for i in range(0,t): #for trump\n",
    "        if i <= a:\n",
    "            train.append((features(trump[i]),\"Trump\"))\n",
    "        else:\n",
    "            test.append((features(trump[i]),\"Trump\"))\n",
    "    for i in range(0,o): #for obama\n",
    "        if i <= b:\n",
    "            train.append((features(obama[i]),\"Obama\"))\n",
    "        else:\n",
    "            test.append((features(obama[i]),\"Obama\"))\n",
    "    for i in range(0,39): #for bush\n",
    "        if i <= 30:\n",
    "            train.append((features(bush[i]),\"Bush\"))\n",
    "        else:\n",
    "            test.append((features(bush[i]),\"Bush\"))\n",
    "    shuffle(train)\n",
    "    shuffle(test)\n",
    "    return train,test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning\n",
    "The main goal of this system is to make good decision about new unseen samples. This is solved by finding weight vectors w and bias b for each of 3 classes. Weights represent importance of each particular feature.<br>\n",
    "Let  $x=(x_1,...,x_n)$  be a feature vector of some sample from training set from class c and a vector of weights $W_c=(w_1,...,w_n)$ and bias  $b_c$  coresponding to the class c. <br> The result of softmax function is probability that sample x belongs to class c :\n",
    "$p(y=c|x)=\\frac{e^{w_c x + b_c}}{\\sum_{k=1}^n e^{w_k x + b_k}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set , test_set = prepare_data() #get the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next thing we need is metric of how good our estimation is , for true class c. This metric/distance is called **cost function**.<br>\n",
    "### Cost function\n",
    "The cost function for a single example $x=(x_1,...,x_n)$ and estimated class  $\\hat{y}$ is the sum of the logs of the K classes:<br>\n",
    "\\begin{equation*} L_{CE}(\\hat{y},y) = -\\sum_{k=1}^n 1\\{y=k\\}log p(y=k|x) \\end{equation*} where $\\{y=k\\}$ is 1 if y = k , else 0. The goal is to find such weights vectors $W_1 , W_2 , W_3$ for 3 classes ,such that cost function is minimized.<br>\n",
    "The gradient for a single sample $x=(x_1,...,x_n)$ is \n",
    "\\begin{equation*}\\frac{\\partial L_{CE}}{\\partial w_k} = (1\\{y = c\\} - p(y=c|x))x_k\\end{equation*} It is the difference between true class $c$ and the probability that model outputs $c$ , multiplied with $x_k$ for each $k = 1,...,n$ <br>\n",
    "### Weights\n",
    "Weights vectors are random numbers for the beggining so that learning is not deterministic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = []  # weights -  the last number for each weight vector is bias for that class\n",
    "learn_rate = 0.09 #set elarning rate beta\n",
    "epochs = 10 #number of epochs\n",
    "def setWeights():  #vectors of weights for each class\n",
    "    w_trump = [uniform(-1,1) for i in range(7)]\n",
    "    w_obama = [uniform(-1,1) for i in range(7)]\n",
    "    w_bush = [uniform(-1,1) for i in range(7)]\n",
    "    w = [w_trump,w_obama,w_bush]  # the last number for each weight vector is bias for that class\n",
    "    return w\n",
    "w = setWeights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning\n",
    "  Turn dataset into feature-set <br>\n",
    "  Initialize vectors of weights for each class , set learning rate $\\beta$ and number of epochs<br>\n",
    "  for each **epoch**<br>\n",
    "  &emsp;&emsp;    shuffle(train-set)<br>\n",
    "       &emsp;&emsp; for each sample $(x^{(i)} =(x_1,...,x_m)  , y^{(i)})$    $i= 1,...,n$ from train-set<br>\n",
    "            &emsp;&emsp;&emsp;&emsp;&emsp; find probabilities that sample belongs to each class <br>\n",
    "            &emsp;&emsp;&emsp;&emsp;&emsp; calculate vector of partial derivations for each class:<br>\n",
    "            &emsp;&emsp;&emsp;&emsp;&emsp; for $j=1,...,m$  $\\frac{\\partial L}{\\partial w_j} = (1\\{\\hat{y} = y^{(i)}\\} - p(\\hat{y} = y^{(i)}|x^{(i)})) x_j$ &emsp; $\\frac{\\partial L}{\\partial b} = 1\\{\\hat{y} = y^{(i)}\\} - p(\\hat{y} = y^{(i)}|x^{(i)})$<br>\n",
    "            &emsp;&emsp;&emsp;&emsp;&emsp;$\\nabla L_C = (\\frac{\\partial L}{\\partial w_1},...,\\frac{\\partial L}{\\partial w_m},\\frac{\\partial L}{\\partial b})$<br>\n",
    "            &emsp;&emsp;&emsp;&emsp;&emsp;$W_C = W_C - \\beta \\nabla L_C$ &emsp;(update weights)<br>\n",
    "  return weights $W_1 , W_2 , W_3$ <br>\n",
    "  \n",
    " ### Learning - example\n",
    "Here is a very simple example of learning process<br>\n",
    "Let's assume that there are 3 classes $c_1 , c_2 , c_3$, that number of epoch is 1 and learning rate is 0.1.For sample x is given: $x = (1,1,2)$ his true class is $c = c_1$. Weights for classes are $w_1 = (0.1,2,1,0), w_2 = (0.5,1,1,1), w_3 = (1,0,0.1,0.3)$. The first thing to do is calculate probabilities for each class,that given sample x belong to each class.<br>\n",
    "For each class $c$ dot product $w_c * x + w_{bias}$ is computed and storaged into vector z. In this case $z = (4.1, 4.5, 1.5)$ and vector of probabilities calculated with softmax function is $(0.389, 0.582, 0.029)$. This means that the most probable class is $c_2$.<br>\n",
    "Next step is to calculate new weights , that is done by computing vector of partial derivations,here we show only for one class , $c_1$. <br>\n",
    "The most important part is the difference between true class $c_1$ and predicted class $c_2$. Because those 2 classes are different, the difference between them is $0 - prob(c_1) = -0.389$. Vector of partial derivations is now $(-0.389, -0.389, -0.778 , -0.389)$ , the last number is partial derivation of the cost function for bias variable. <br>\n",
    "The last thing to do is update weights for class $c_1$ : $w_1 = (0.1-0.1*(-0.389),2-0.1*(-0.389),1-0.1*(-0.778),0-0.1*(-0.389)) = (0.1389, 2.0389, 1.0778, 0.0389)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    w = setWeights()\n",
    "    for epoch in range(epochs):\n",
    "        shuffle(train_set)\n",
    "        for sample in train_set:\n",
    "            f = sample[0] #features of sample\n",
    "            z = [0,0,0] # to calculate probability for each class\n",
    "            for j in range(0,3):# for each class\n",
    "                z[j] = scalar(w[j][:6],f) + w[j][6]\n",
    "                if z[j] > 705: # beacuse of the overflow that could happen in softmax function\n",
    "                    z[j] = 705\n",
    "            prob = softmax(z) # vector of probabilities for each class,that sample belong to those classes\n",
    "            c = sample[1] # true class for given sample\n",
    "            # compute vector of partial dervations for each class\n",
    "            for j in range(0,3):\n",
    "                dif = (1 if c == classes[j] else 0) - prob[j] # difference between true class and some given class\n",
    "                grad = [dif * x for x in f] # vector of partial derivatives of cost function\n",
    "                grad.append(dif) # partial derivative for \"bias\" variable\n",
    "                w[j] = [x - learn_rate * y for x,y in zip(w[j],grad)] #update weights\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning explained\n",
    "Stohastic gradient descent algorithm is used for learning. It is numerical method used in cases when trying to find minimum of the function is analytically harder than finding it with this iterative method.<br>\n",
    "It is convenient to use this algorithm for convex functions because those kind of functions have only 1 minimum (global minimum) and algorithm can't get stuck in the local minima.<br>\n",
    "For each sample from train-set , first thing is to find probabilities that sample belongs to each of 3 classes. After that , algorithm change weights for classes depending on quality of predicting true class of the given sample.<br>\n",
    "The key part is $1\\{\\hat{y} = y^{(i)}\\} - p(\\hat{y} = y^{(i)}|x^{(i)})$. It means that in the case when true class is predicted correctly , if $p(\\hat{y} = y^{(i)}|x^{(i)})$ is high (close to 1) there is no need to change weights much for that class , because good estimation is made. Otherwise , if the class estimation is wrong ,but $p(\\hat{y} = y^{(i)}|x^{(i)})$ is very low(close to 0) there is also no need to change weights much for that class , because model will make mistake with very small probability. <br> In all other cases , bigger change for weights is needed because model will make mistake with higher probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "Model evaluation parameters that are used are **precision** , **recall** and **f1** .<br>\n",
    "**tp** - True positive<br>\n",
    "**fp** - False positive<br>\n",
    "**tn** - True negative<br>\n",
    "**fn** - False negative<br>\n",
    "Separate contigency tables are created for each of 3 classes and then joined table is made.<br>\n",
    "$precision = \\frac{tp}{tp+fp}$<br>\n",
    "$recall = \\frac{tp}{tp+tn}$  &emsp;  $f1 = \\frac{2 * precision * recall}{precision+recall}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(weights,features):\n",
    "    z = [0,0,0] # calculate probability for each class\n",
    "    for j in range(0,3):# for each class\n",
    "        z[j] = scalar(weights[j][:6],features) + weights[j][6]\n",
    "        if z[j] > 705: # beacuse of the overflow that could happen in softmax function\n",
    "            z[j] = 705\n",
    "    prob = softmax(z)\n",
    "    return classes[prob.index(max(prob))] #look for class with the highest probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#contigency tables for each class - first is tp , second is fp , third is tn and the last is fn\n",
    "ctables = [[0,0,0,0],[0,0,0,0],[0,0,0,0]] # first for trump,second for obama and last for bush"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(weights):\n",
    "    for (X,c) in test_set: # X is the sample and c is true class\n",
    "        y = classify(weights,X) # predicted class for sample X\n",
    "        for j in range(0,3): # for each class\n",
    "            if classes[j] == y :\n",
    "                if y == c: #if the predicted class is the true class of the sample\n",
    "                    ctables[j][0] +=1 #true positive\n",
    "                else:\n",
    "                    ctables[j][1] +=1 #false positive\n",
    "            else:\n",
    "                if y == c:\n",
    "                    ctables[j][3] += 1 #false negative\n",
    "                else: # if this class is not predicted , but it shouldn't be predicted\n",
    "                    ctables[j][2] += 1 #true negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(train())\n",
    "#calculate precision,recall and f-measure\n",
    "tp = ctables[0][0]+ctables[1][0]+ctables[2][0]\n",
    "fp = ctables[0][1]+ctables[1][1]+ctables[2][1]\n",
    "tn = ctables[0][2]+ctables[1][2]+ctables[2][2]\n",
    "fn = ctables[0][3]+ctables[1][3]+ctables[2][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = tp/(tp+fp) # precision\n",
    "r = tp/(tp+tn) # recall\n",
    "f = (2*p*r)/(p+r) # f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "The point of this project is implementing NLP and machine learning techniques learned on Natural language processing course. Evaluation parameters are not satisfying because very simple language features are used in this example. For getting much more better evaluation parameters it is needed to include some specifical language characteristics that have greater impact on statistical learning than this features.The second point is to understand and implement one optimization algorithm , in this case stohastic gradient decent and explain his role in learning - finding the best weights for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
